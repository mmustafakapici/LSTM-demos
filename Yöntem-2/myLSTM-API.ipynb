{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf # yfinance is used to download the data from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_data  = yf.download('BTC-USD' , period='3600d')\n",
    "df = btc_data[['Open', 'High', 'Low', 'Close' , 'Adj Close', 'Volume']]  # İlgili sütunların seçilmesi\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # Veri setinin bilgileri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)  # Veri setinin uzunluğu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want closing price for each day \n",
    "bitcoin_prices = pd.DataFrame(df[\"Close\"]).rename(columns={\"Close\": \"Price\"}) # Sadece kapanış fiyatlarını alıyoruz ve sütun ismini değiştiriyoruz\n",
    "bitcoin_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices.plot(figsize=(10, 7))\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting historical Bitcoin data with Python\n",
    "from datetime import datetime\n",
    "\n",
    "timesteps = []\n",
    "btc_price = []\n",
    "\n",
    "\n",
    "# ilk 10 satırın gösterilmesi\n",
    "for row in df.itertuples():\n",
    "  timesteps.append(row.Index)\n",
    "  btc_price.append(row.Close)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timesteps[:10], btc_price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(timesteps, btc_price)\n",
    "plt.title(\"Price of Bitcoin \", fontsize=16)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC Price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bitcoin date array\n",
    "timesteps = bitcoin_prices.index.to_numpy()\n",
    "prices = bitcoin_prices[\"Price\"].to_numpy()\n",
    "\n",
    "timesteps[:10], prices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits the right way for time series data\n",
    "split_size = int(0.8 * len(prices)) # 80% train, 20% test\n",
    "\n",
    "# Create train data splits (everything before the split)\n",
    "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
    "\n",
    "# Create test data splits (everything after the split)\n",
    "X_test, y_test = timesteps[split_size:], prices[split_size:]\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correctly made splits\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train, y_train, s=5, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot time series data\n",
    "def plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):\n",
    "  \"\"\"\n",
    "  Plots a timesteps (a series of points in time) against values (a series of values across timesteps).\n",
    "  \n",
    "  Parameters\n",
    "  ---------\n",
    "  timesteps : array of timesteps\n",
    "  values : array of values across time\n",
    "  format : style of plot, default \".\"\n",
    "  start : where to start the plot (setting a value will index from start of timesteps & values)\n",
    "  end : where to end the plot (setting a value will index from end of timesteps & values)\n",
    "  label : label to show on plot of values\n",
    "  \"\"\"\n",
    "  # Plot the series\n",
    "  plt.plot(timesteps[start:end], values[start:end], format, label=label)\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(\"BTC Price\")\n",
    "  if label:\n",
    "    plt.legend(fontsize=14) # make label bigger\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out our plotting function\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\n",
    "plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  \n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy()\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average price of Bitcoin in test dataset\n",
    "tf.reduce_mean(y_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1 # predict 1 step at a time\n",
    "WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=1):\n",
    "  \"\"\"\n",
    "  Creates labels for windowed dataset.\n",
    "\n",
    "  E.g. if horizon=1 (default)\n",
    "  Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
    "  \"\"\"\n",
    "  return x[:, :-horizon], x[:, -horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the window labelling function\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to view NumPy arrays as windows \n",
    "def make_windows(x, window_size=7, horizon=1):\n",
    "  \"\"\"\n",
    "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "  \"\"\"\n",
    "  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
    "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "  # print(f\"Window step:\\n {window_step}\")\n",
    "\n",
    "  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
    "  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
    "\n",
    "  # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
    "  windowed_array = x[window_indexes]\n",
    "\n",
    "  # 4. Get the labelled windows\n",
    "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "\n",
    "  return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#örnek 3 satırın düzenli şekilde gösterilmesi\n",
    "for i in range(3):\n",
    "  print(f\"Window: {full_windows[i]} \\n -> Label: {full_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 3 windows/labels\n",
    "for i in range(3):\n",
    "  print(f\"Window: {full_windows[i-3]}  \\n -> Label: {full_labels[i-3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning windows into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train/test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "  \"\"\"\n",
    "  Splits matching pairs of windows and labels into train and test splits.\n",
    "  \"\"\"\n",
    "  split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n",
    "  train_windows = windows[:split_size]\n",
    "  train_labels = labels[:split_size]\n",
    "  test_windows = windows[split_size:]\n",
    "  test_labels = labels[split_size:]\n",
    "  return train_windows, test_windows, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if same (accounting for horizon and window size)\n",
    "np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a function to implement a ModelCheckpoint callback with a specific filename \n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                            verbose=0, # only output a limited amount of text\n",
    "                                            save_best_only=True) # save only the best model to file\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Construct model\n",
    "model_1 = tf.keras.Sequential([\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation                        \n",
    "], name=\"model_1_dense\") # give the model a name so we can save it\n",
    "\n",
    "# Compile model\n",
    "model_1.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n",
    "\n",
    "# Fit model\n",
    "model_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices\n",
    "            y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_1.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate model on test data\n",
    "model_1.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in saved best performing model_1 and evaluate on test data\n",
    "model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\n",
    "model_1.evaluate(test_windows, test_labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_preds(model, input_data):\n",
    "  \"\"\"\n",
    "  Uses model to make predictions on input_data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model: trained model \n",
    "  input_data: windowed input data (same kind of data model was trained on)\n",
    "\n",
    "  Returns model predictions on input_data.\n",
    "  \"\"\"\n",
    "  forecast = model.predict(input_data)\n",
    "  return tf.squeeze(forecast) # return 1D array of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions using model_1 on the test dataset and view the results\n",
    "model_1_preds = make_preds(model_1, test_windows)\n",
    "len(model_1_preds), model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate preds\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 300\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Account for the test_window offset and index into test_labels to ensure correct plotting\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 DENSE (win = 30 horizon = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7\n",
    "WINDOW_SIZE = 30\n",
    "\n",
    "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create model (same as model_1 except with different data input size)\n",
    "model_3 = tf.keras.Sequential([\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(HORIZON)\n",
    "], name=\"model_3_dense\")\n",
    "\n",
    "model_3.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "model_3.fit(train_windows,\n",
    "            train_labels,\n",
    "            batch_size=128,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_3.name)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How did our model with a larger window size and horizon go?\n",
    "model_3.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load in best version of model_3 and evaluate\n",
    "model_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\")\n",
    "model_3.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The predictions are going to be 7 steps at a time (this is the HORIZON size)\n",
    "model_3_preds = make_preds(model_3,\n",
    "                           input_data=test_windows)\n",
    "model_3_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model_3 results - these are going to be multi-dimensional because\n",
    "# we're trying to predict more than one step at a time.\n",
    "model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "  # Account for different sized metrics (for longer horizons, reduce to single number)\n",
    "  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n",
    "    mae = tf.reduce_mean(mae)\n",
    "    mse = tf.reduce_mean(mse)\n",
    "    rmse = tf.reduce_mean(rmse)\n",
    "    mape = tf.reduce_mean(mape)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model_3 results aggregated to single values\n",
    "model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_3_preds)\n",
    "model_3_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 300\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\n",
    "# Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot)\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label=\"model_3_preds\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 300\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data)\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], \n",
    "                 values=test_labels[:, 0], \n",
    "                 start=offset, \n",
    "                 label=\"Test_data\")\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):], \n",
    "                 values=tf.reduce_mean(model_3_preds, axis=1), \n",
    "                 format=\"-\",\n",
    "                 start=offset, \n",
    "                 label=\"model_3_preds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "              \"horizon_1_window_7\": model_1_results[\"mae\"], \n",
    "               \n",
    "              \"horizon_7_window_30\": model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5  RNN (LSTM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Let's build an LSTM model with the Functional API\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n",
    "# print(x.shape)\n",
    "# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\n",
    "x = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error\n",
    "# print(x.shape)\n",
    "# Add another optional dense layer (you could add more of these to see if they improve model performance)\n",
    "# x = layers.Dense(32, activation=\"relu\")(x)\n",
    "output = layers.Dense(HORIZON)(x)\n",
    "model_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554 \n",
    "model_5.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_5.name)])\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load in best version of model 5 and evaluate on the test data\n",
    "model_5 = tf.keras.models.load_model(\"model_experiments/model_5_lstm/\")\n",
    "model_5.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions with our LSTM model\n",
    "model_5_preds = make_preds(model_5, test_windows)\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model 5 preds\n",
    "model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_5_preds)\n",
    "model_5_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's make a multivariate time series\n",
    "bitcoin_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block reward values\n",
    "block_reward_1 = 50 # 3 January 2009 (2009-01-03) - this block reward isn't in our dataset (it starts from 01 October 2013)\n",
    "block_reward_2 = 25 # 28 November 2012 \n",
    "block_reward_3 = 12.5 # 9 July 2016\n",
    "block_reward_4 = 6.25 # 11 May 2020\n",
    "\n",
    "# Block reward dates (datetime form of the above date stamps)\n",
    "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
    "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
    "block_reward_4_datetime = np.datetime64(\"2020-05-11\")\n",
    "\n",
    "\n",
    "\n",
    "# Get date indexes for when to add in different block dates\n",
    "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
    "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\n",
    "#block_reward_2_days, block_reward_3_days\n",
    "\n",
    "\n",
    "# Add block_reward column\n",
    "bitcoin_prices_block = bitcoin_prices.copy()\n",
    "bitcoin_prices_block[\"block_reward\"] = None\n",
    "\n",
    "# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\n",
    "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
    "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\n",
    "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\n",
    "bitcoin_prices_block.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the block reward/price over time\n",
    "# Note: Because of the different scales of our values we'll scale them to be between 0 and 1.\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"]]), # we need to scale the data first\n",
    "                                     columns=bitcoin_prices_block.columns,\n",
    "                                     index=bitcoin_prices_block.index)\n",
    "scaled_price_block_df.plot(figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup dataset hyperparameters\n",
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the Bitcoin historical data with block reward feature\n",
    "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
    "\n",
    "# Add windowed columns\n",
    "for i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE\n",
    "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
    "bitcoin_prices_windowed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors \n",
    "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32) \n",
    "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View labels\n",
    "y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make train and test sets\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 6 Multivariate time series (DENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Make multivariate time series model\n",
    "model_6 = tf.keras.Sequential([\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model\n",
    "  layers.Dense(HORIZON)\n",
    "], name=\"model_6_dense_multivariate\")\n",
    "\n",
    "# Compile\n",
    "model_6.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit\n",
    "model_6.fit(X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            verbose=0, # only print 1 line per epoch\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_6.name)])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure best model is loaded and evaluate\n",
    "model_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\")\n",
    "model_6.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on multivariate data\n",
    "model_6_preds = tf.squeeze(model_6.predict(X_test))\n",
    "model_6_preds[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate preds\n",
    "model_6_results = evaluate_preds(y_true=y_test,\n",
    "                                 y_pred=model_6_preds)\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9: Train a model on the full historical data to make predictions into future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices_windowed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on entire data to make prediction for the next day \n",
    "X_all = bitcoin_prices_windowed.drop([\"Price\", \"block_reward\"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model\n",
    "y_all = bitcoin_prices_windowed.dropna()[\"Price\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Turn X and y into tensor Datasets\n",
    "features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)\n",
    "labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)\n",
    "\n",
    "# 2. Combine features & labels\n",
    "dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))\n",
    "\n",
    "# 3. Batch and prefetch for optimal performance\n",
    "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
    "dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create model (nice and simple, just to test)\n",
    "model_9 = tf.keras.Sequential([\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(HORIZON)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model_9.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit model on all of the data to make future forecasts\n",
    "model_9.fit(dataset_all,\n",
    "            epochs=100,\n",
    "            verbose=0) # don't print out anything, we've seen this all before\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many timesteps to predict into the future?\n",
    "INTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create function to make predictions into the future\n",
    "def make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -> list:\n",
    "  \"\"\"\n",
    "  Makes future forecasts into_future steps after values ends.\n",
    "\n",
    "  Returns future forecasts as list of floats.\n",
    "  \"\"\"\n",
    "  # 2. Make an empty list for future forecasts/prepare data to forecast on\n",
    "  future_forecast = []\n",
    "  last_window = values[-WINDOW_SIZE:] # only want preds from the last window (this will get updated)\n",
    "\n",
    "  # 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time \n",
    "  for _ in range(into_future):\n",
    "    \n",
    "    # Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)\n",
    "    future_pred = model.predict(tf.expand_dims(last_window, axis=0))\n",
    "    print(f\"Predicting on: \\n {last_window} -> Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n",
    "    \n",
    "    # Append predictions to future_forecast\n",
    "    future_forecast.append(tf.squeeze(future_pred).numpy())\n",
    "    # print(future_forecast)\n",
    "\n",
    "    # Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)\n",
    "    last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]\n",
    "  \n",
    "  return future_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make forecasts into future of the price of Bitcoin\n",
    "# Note: if you're reading this at a later date, you may already be in the future, so the forecasts \n",
    "# we're making may not actually be forecasts, if that's the case, readjust the training data.\n",
    "future_forecast = make_future_forecast(values=y_all,\n",
    "                                       model=model_9,\n",
    "                                       into_future=INTO_FUTURE,\n",
    "                                       window_size=WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_forecast[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_dates(start_date, into_future, offset=1):\n",
    "  \"\"\"\n",
    "  Returns array of datetime values from ranging from start_date to start_date+horizon.\n",
    "\n",
    "  start_date: date to start range (np.datetime64)\n",
    "  into_future: number of days to add onto start date for range (int)\n",
    "  offset: number of days to offset start_date by (default 1)\n",
    "  \"\"\"\n",
    "  start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \"D\" stands for day\n",
    "  end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date\n",
    "  return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Last timestep of timesteps (currently in np.datetime64 format)\n",
    "last_timestep = bitcoin_prices.index[-1]\n",
    "last_timestep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get next two weeks of timesteps\n",
    "next_time_steps = get_future_dates(start_date=last_timestep, \n",
    "                                   into_future=INTO_FUTURE)\n",
    "next_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert last timestep/final price so the graph doesn't look messed\n",
    "next_time_steps = np.insert(next_time_steps, 0, last_timestep)\n",
    "future_forecast = np.insert(future_forecast, 0, btc_price[-1])\n",
    "next_time_steps, future_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot future price predictions of Bitcoin\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_time_series(bitcoin_prices.index, btc_price, start=2500, format=\"-\", label=\"Actual BTC Price\")\n",
    "plot_time_series(next_time_steps, future_forecast, format=\"-\", label=\"Predicted BTC Price\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#future forecastın tarihlerini ve fiyatları bana yaz\n",
    "future_forecast = pd.DataFrame(future_forecast, columns=[\"Price\"])\n",
    "future_forecast[\"Date\"] = next_time_steps\n",
    "future_forecast.set_index(\"Date\", inplace=True)\n",
    "future_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple oluştur  : date ve price\n",
    "# tuple JSON a çevir\n",
    "\n",
    "future_forecast.to_json(\"future_forecast.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
