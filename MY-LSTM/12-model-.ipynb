{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.711861Z",
     "start_time": "2024-02-05T19:03:37.624391Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# yeni hali \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  LSTM , BatchNormalization\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector, Concatenate, Attention , Permute , Flatten , Activation , Multiply , Lambda\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.746017Z",
     "start_time": "2024-02-05T19:03:37.719010Z"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('datas'):\n",
    "    #sil\n",
    "    os.system('rm -rf datas')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.831577Z",
     "start_time": "2024-02-05T19:03:37.773728Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.865090Z",
     "start_time": "2024-02-05T19:03:37.852550Z"
    }
   },
   "outputs": [],
   "source": [
    "#period = '3000d'\n",
    "period = 'max'\n",
    "\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Close','Adj Close', 'Volume']\n",
    "#features = [ 'Close' ]  # Sadece kapanış fiyatını kullanın\n",
    "\n",
    "#tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'GOOGL', 'TSLA', 'NVDA', 'PYPL', 'ADBE','BTC-USD', 'ETH-USD', 'XRP-USD', 'LTC-USD','BCH-USD', 'BNB-USD', 'LINK-USD', 'ADA-USD', 'XLM-USD', 'SOL-USD', 'TRX-USD']\n",
    "\n",
    "tickers = ['SOL-USD']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.890218Z",
     "start_time": "2024-02-05T19:03:37.872964Z"
    }
   },
   "outputs": [],
   "source": [
    "def indir_ve_df_olustur(tickers, period):\n",
    "    # Her bir hisse senedi için boş bir sözlük oluşturun\n",
    "    pariteler = {}\n",
    "\n",
    "    # Her bir hisse senedi için döngü oluşturun ve verileri indirin\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Hisse senedi verilerini indirin\n",
    "            veri = yf.download(ticker, period=period)\n",
    "\n",
    "            veri = veri[features]   \n",
    "            # Veriyi sözlüğe ekleyin\n",
    "            pariteler[ticker] = veri\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} için veri indirilirken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "    # Her bir hisse senedi için ayrı bir veri çerçevesi oluşturun\n",
    "    df_listesi = [veri for veri in pariteler.values()]\n",
    "\n",
    "    return df_listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:37.906581Z",
     "start_time": "2024-02-05T19:03:37.893733Z"
    }
   },
   "outputs": [],
   "source": [
    "#pariteler adında klasör oluştur :\n",
    "\n",
    "import os \n",
    "\n",
    "if not os.path.exists('datas/pariteler'):\n",
    "    os.makedirs('datas/pariteler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.325991Z",
     "start_time": "2024-02-05T19:03:37.912648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "SOL-USD verisi csv olarak kaydedildi\n"
     ]
    }
   ],
   "source": [
    "datasets = indir_ve_df_olustur(tickers, period)\n",
    "\n",
    "#pariteler içine kaydet \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].to_csv(f'datas/pariteler/{tickers[i]}.csv')\n",
    "    print(f'{tickers[i]} verisi csv olarak kaydedildi')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOL-USD verisi scaled edilerek kaydedildi\n"
     ]
    }
   ],
   "source": [
    "scaler  = MinMaxScaler()\n",
    "\n",
    "#her bir veri çerçevesi için scaled dataset oluştur\n",
    "\n",
    "if not os.path.exists('datas/scaled_datasets'):\n",
    "    os.makedirs('datas/scaled_datasets')\n",
    "\n",
    "scaled_datasets = []\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    scaled_datasets.append(scaler.fit_transform(datasets[i]))\n",
    "    np.save(f'datas/scaled_datasets/{tickers[i]}.npy', scaled_datasets[i])\n",
    "    print(f'{tickers[i]} verisi scaled edilerek kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.340082Z",
     "start_time": "2024-02-05T19:03:38.314689Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOL-USD verisi dizilere bölünerek kaydedildi\n"
     ]
    }
   ],
   "source": [
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "seq_length = 60  # Son 60 günü kullanarak\n",
    "pred_length = 7  # Önümüzdeki 7 günü tahmin etme\n",
    "\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length-pred_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "if not os.path.exists('datas/sequences'):\n",
    "    os.makedirs('datas/sequences')\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    X_, y_ = create_sequences(scaled_datasets[i], seq_length, pred_length)\n",
    "    X.append(X_)\n",
    "    y.append(y_)\n",
    "    np.save(f'datas/sequences/X_{tickers[i]}.npy', X_)\n",
    "    np.save(f'datas/sequences/y_{tickers[i]}.npy', y_)\n",
    "    print(f'{tickers[i]} verisi dizilere bölünerek kaydedildi')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.401439Z",
     "start_time": "2024-02-05T19:03:38.327024Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için  seq2seq attention modeli oluştur\n",
    "\n",
    "\n",
    "input_shape = (seq_length, len(features))\n",
    "output_shape = (pred_length, len(features))\n",
    "\n",
    "def create_model(input_shape, output_shape):\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    encoder = LSTM(100, return_sequences=True, return_state=True)(encoder_inputs)\n",
    "    encoder_last_h, encoder_last_c = encoder[:,-1, :-1], encoder[:,-1, -1]  # Son LSTM hücresinin durumu\n",
    "    decoder_inputs = RepeatVector(output_shape[0])(encoder_last_h)\n",
    "    decoder = LSTM(100, return_sequences=True)(decoder_inputs, initial_state=[encoder_last_h, encoder_last_c])\n",
    "    attention = Concatenate()([encoder, decoder])\n",
    "    attention = TimeDistributed(Dense(1, activation='tanh'))(attention)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(100)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    sent_representation = Multiply()([encoder, attention])\n",
    "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(100,))(sent_representation)\n",
    "    decoder = Concatenate()([decoder, sent_representation])\n",
    "    decoder = TimeDistributed(Dense(100, activation='tanh'))(decoder)\n",
    "    outputs = TimeDistributed(Dense(output_shape[1], activation='linear'))(decoder)\n",
    "    model = Model(encoder_inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.417423Z",
     "start_time": "2024-02-05T19:03:38.348577Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.420762Z",
     "start_time": "2024-02-05T19:03:38.351525Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(scaled_datasets)):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# modeli oluştur\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# modeli derle\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[21], line 10\u001B[0m, in \u001B[0;36mcreate_model\u001B[0;34m(input_shape, output_shape)\u001B[0m\n\u001B[1;32m      8\u001B[0m encoder_inputs \u001B[38;5;241m=\u001B[39m Input(shape\u001B[38;5;241m=\u001B[39minput_shape)\n\u001B[1;32m      9\u001B[0m encoder \u001B[38;5;241m=\u001B[39m LSTM(\u001B[38;5;241m100\u001B[39m, return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)(encoder_inputs)\n\u001B[0;32m---> 10\u001B[0m encoder_last_h, encoder_last_c \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m, encoder[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# Son LSTM hücresinin durumu\u001B[39;00m\n\u001B[1;32m     11\u001B[0m decoder_inputs \u001B[38;5;241m=\u001B[39m RepeatVector(output_shape[\u001B[38;5;241m0\u001B[39m])(encoder_last_h)\n\u001B[1;32m     12\u001B[0m decoder \u001B[38;5;241m=\u001B[39m LSTM(\u001B[38;5;241m100\u001B[39m, return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)(decoder_inputs, initial_state\u001B[38;5;241m=\u001B[39m[encoder_last_h, encoder_last_c])\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# her bir veri çerçevesi için modeli oluştur ve eğit\n",
    "\n",
    "if not os.path.exists('datas/models'):\n",
    "    os.makedirs('datas/models')\n",
    "\n",
    "models = []\n",
    "batch_size = 60\n",
    "epochs = 20\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    # modeli oluştur\n",
    "    model = create_model(input_shape, output_shape)\n",
    "    \n",
    "    # modeli derle\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # modeli listeye ekle\n",
    "    models.append(model)\n",
    "    \n",
    "    # modeli eğit\n",
    "    models[i].fit(X[i], y[i],\n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  \n",
    "                  #validation_split=0.2, \n",
    "                  \n",
    "                  verbose=1)\n",
    "    \n",
    "    # modeli kaydet\n",
    "    models[i].save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.756723Z",
     "start_time": "2024-02-05T19:03:38.360252Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# modelerden bir tanesini png kaydedelim\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(models[0], to_file='datas/model.png', show_shapes=True, show_layer_names=True , rankdir='TB', expand_nested=True, dpi=96, )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.799306Z",
     "start_time": "2024-02-05T19:03:38.759917Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model summary \n",
    "models[0].summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.762430Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO : \n",
    "# soru = bu none girişler nereden geliyor ?\n",
    "# cevap = "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.764866Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için last_sequence = data_scaled[-seq_length:] mantığında prediction yap\n",
    "\n",
    "if not os.path.exists('datas/predictions'):\n",
    "    os.makedirs('datas/predictions')\n",
    "    \n",
    "predictions = []    \n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    last_sequence = scaled_datasets[i][-seq_length:]\n",
    "    last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    # buradaki 1 seq_length, len(features) şeklinde olması ne işe yarıyor : ? \n",
    "    # cevap = \n",
    "    predicted = models[i].predict(last_sequence)\n",
    "    \n",
    "    predictions.append(predicted)\n",
    "    np.save(f'datas/predictions/{tickers[i]}_prediction.npy', predicted)\n",
    "    print(f'{tickers[i]} verisi için prediction yapıldı ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.768808Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için bütün sütunları predictionları geri dönüşüm yap ve yeni bi df e kaydet\n",
    "\n",
    "if not os.path.exists('datas/forecasts'):\n",
    "    os.makedirs('datas/forecasts')\n",
    "    \n",
    "forecasts = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    prediction = np.load(f'datas/predictions/{tickers[i]}_prediction.npy')\n",
    "    \n",
    "    \n",
    "    prediction = scaler.inverse_transform(prediction.reshape(-1, len(features) ))\n",
    "    forecasts.append(prediction)\n",
    "    np.save(f'datas/forecasts/{tickers[i]}_forecast.npy', prediction)\n",
    "    print(f'{tickers[i]} verisi için forecast yapıldı ve kaydedildi')\n",
    "    print(forecasts[i].shape)\n",
    "    \n",
    "# dataframe oluştur\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    forecast = forecasts[i]\n",
    "    forecast_dates = datasets[i].index[-pred_length:]\n",
    "    forecast = pd.DataFrame(forecast, index=forecast_dates, columns=features)\n",
    "    forecast.to_csv(f'datas/forecasts/{tickers[i]}_forecast.csv')\n",
    "    print(f'{tickers[i]} verisi için forecast dataframe oluşturuldu ve kaydedildi')\n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.771253Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#dataframeleri bi yazdır tarihleri ile\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    forecast = pd.read_csv(f'datas/forecasts/{tickers[i]}_forecast.csv', index_col=0)\n",
    "    print(f'{tickers[i]} verisi için forecast dataframe')\n",
    "    print(forecast)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.773744Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Gerçek veriyi yazdır aynı tarihteki \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(f'{tickers[i]} verisi için gerçek veri')\n",
    "    print(datasets[i].tail(pred_length))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.776414Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Kısım"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için eğitime uygun full data hazırla (tensor dataset)\n",
    "\n",
    "if not os.path.exists('datas/full_data'):\n",
    "    os.makedirs('datas/full_data')\n",
    "    \n",
    "full_data = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    \n",
    "    scaled_dataset_all = np.load(f'datas/scaled_datasets/{tickers[i]}.npy')\n",
    "    \n",
    "    scaled_dataset_all = tf.data.Dataset.from_tensor_slices(scaled_dataset_all)\n",
    "    \n",
    "    scaled_dataset_all = tf.data.Dataset.zip((scaled_dataset_all, scaled_dataset_all.skip(seq_length)))\n",
    "    \n",
    "    scaled_dataset_all = scaled_dataset_all.batch(seq_length, drop_remainder=True).prefetch(1)\n",
    "\n",
    "    scaled_dataset_all = scaled_dataset_all.cache(f'datas/full_data/{tickers[i]}_full_data')\n",
    "\n",
    "\n",
    "    \n",
    "    full_data.append(scaled_dataset_all)\n",
    "    \n",
    "    #full_data[i] = full_data[i].batch(seq_length, drop_remainder=True).prefetch(1)\n",
    "    \n",
    "    #full_data[i] = full_data[i].cache(f'datas/full_data/{tickers[i]}_full_data')\n",
    "\n",
    "    print(f'{tickers[i]} verisi için full data hazırlandı')\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.778892Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder_inputs.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.781448Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.787106Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli import et ve eğit\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    model = tf.keras.models.load_model(f'datas/models/{tickers[i]}_model.h5', custom_objects={'Attention': Attention})\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    \n",
    "    model.fit(full_data[i], epochs=epochs, \n",
    "              \n",
    "              verbose=1)\n",
    "    \n",
    "    \n",
    "    model.save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.789731Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "into_future = 1\n",
    "\n",
    "def future_forecast(model, data, into_future):\n",
    "    future_forecast = []\n",
    "    last_sequence = data[-seq_length:]\n",
    "    last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    for i in range(into_future):\n",
    "        predicted = model.predict(last_sequence)\n",
    "        future_forecast.append(predicted)\n",
    "        last_sequence = np.append(last_sequence, predicted)\n",
    "        last_sequence = last_sequence[-seq_length:]\n",
    "        last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    return future_forecast\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.792480Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print( into_future , seq_length,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.795072Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "future_values = []\n",
    "\n",
    "if not os.path.exists('datas/future_values'):\n",
    "    os.makedirs('datas/future_values')\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    future_values.append(future_forecast(models[i], scaled_datasets[i], into_future))\n",
    "    np.save(f'datas/future_values/{tickers[i]}_future_values.npy', future_values[i])\n",
    "    print(f'{tickers[i]} verisi için future values yapıldı ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.796679Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için future valuesları geri dönüşüm yap ve yeni bi df e kaydet\n",
    "\n",
    "if not os.path.exists('datas/future_forecasts'):\n",
    "    os.makedirs('datas/future_forecasts')\n",
    "    \n",
    "future_forecasts = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    \n",
    "    #np array olarak future values al\n",
    "    future_value = np.load(f'datas/future_values/{tickers[i]}_future_values.npy')\n",
    "    future_value = scaler.inverse_transform(future_value.reshape(-1, len(features) ))\n",
    "    future_forecasts.append(future_value)\n",
    "    np.save(f'datas/future_forecasts/{tickers[i]}_future_forecast.npy', future_value)\n",
    "    print(f'{tickers[i]} verisi için future forecast yapıldı ve kaydedildi')\n",
    "    print(future_forecasts[i].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.798552Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(future_forecasts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:38.818769Z",
     "start_time": "2024-02-05T19:03:38.800622Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(into_future) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.802976Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# dataframe oluştur\n",
    "\n",
    "if not os.path.exists('datas/future_forecasts_df'):\n",
    "    os.makedirs('datas/future_forecasts_df')\n",
    "\n",
    "hafta = (into_future*7) + 1\n",
    "\n",
    "for i in range(len(future_forecasts)):\n",
    "    future_forecast = future_forecasts[i]\n",
    "    future_forecast_dates = pd.date_range(start=datasets[i].index[-1], periods=hafta)[1:]\n",
    "    future_forecast = pd.DataFrame(future_forecast, index=future_forecast_dates, columns=features)\n",
    "    future_forecast.to_csv(f'datas/future_forecasts_df/{tickers[i]}_future_forecast.csv')\n",
    "    print(f'{tickers[i]} verisi için future forecast dataframe oluşturuldu ve kaydedildi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.806859Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# future forecast dataframe leri yazdır\n",
    "\n",
    "for i in range(len(future_forecasts)):\n",
    "    future_forecast = pd.read_csv(f'datas/future_forecasts_df/{tickers[i]}_future_forecast.csv', index_col=0)\n",
    "    print(f'{tickers[i]} verisi için future forecast dataframe')\n",
    "    print(future_forecast)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.808378Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T19:03:38.810136Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
