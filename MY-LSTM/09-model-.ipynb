{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yeni hali \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  LSTM , BatchNormalization\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector, Concatenate, Attention\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('datas'):\n",
    "    #sil\n",
    "    os.system('rm -rf datas')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#period = '3000d'\n",
    "period = 'max'\n",
    "\n",
    "\n",
    "#features = ['Open', 'High', 'Low', 'Close','Adj Close', 'Volume']\n",
    "features = [ 'Close' ]  # Sadece kapanış fiyatını kullanın\n",
    "\n",
    "tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'GOOGL', 'TSLA', 'NVDA', 'PYPL', 'ADBE','BTC-USD', 'ETH-USD', 'XRP-USD', 'LTC-USD','BCH-USD', 'BNB-USD', 'LINK-USD', 'ADA-USD', 'XLM-USD', 'SOL-USD', 'TRX-USD']\n",
    "\n",
    "#tickers = ['AAPL-USD']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indir_ve_df_olustur(tickers, period):\n",
    "    # Her bir hisse senedi için boş bir sözlük oluşturun\n",
    "    pariteler = {}\n",
    "\n",
    "    # Her bir hisse senedi için döngü oluşturun ve verileri indirin\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Hisse senedi verilerini indirin\n",
    "            veri = yf.download(ticker, period=period)\n",
    "\n",
    "            veri = veri[features]   \n",
    "            # Veriyi sözlüğe ekleyin\n",
    "            pariteler[ticker] = veri\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} için veri indirilirken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "    # Her bir hisse senedi için ayrı bir veri çerçevesi oluşturun\n",
    "    df_listesi = [veri for veri in pariteler.values()]\n",
    "\n",
    "    return df_listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pariteler adında klasör oluştur :\n",
    "\n",
    "import os \n",
    "\n",
    "if not os.path.exists('datas/pariteler'):\n",
    "    os.makedirs('datas/pariteler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = indir_ve_df_olustur(tickers, period)\n",
    "\n",
    "#pariteler içine kaydet \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].to_csv(f'datas/pariteler/{tickers[i]}.csv')\n",
    "    print(f'{tickers[i]} verisi csv olarak kaydedildi')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler  = MinMaxScaler()\n",
    "\n",
    "#her bir veri çerçevesi için scaled dataset oluştur\n",
    "\n",
    "if not os.path.exists('datas/scaled_datasets'):\n",
    "    os.makedirs('datas/scaled_datasets')\n",
    "\n",
    "scaled_datasets = []\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    scaled_datasets.append(scaler.fit_transform(datasets[i]))\n",
    "    np.save(f'datas/scaled_datasets/{tickers[i]}.npy', scaled_datasets[i])\n",
    "    print(f'{tickers[i]} verisi scaled edilerek kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "seq_length = 60  # Son 60 günü kullanarak\n",
    "pred_length = 7  # Önümüzdeki 7 günü tahmin etme\n",
    "\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length-pred_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "if not os.path.exists('datas/sequences'):\n",
    "    os.makedirs('datas/sequences')\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    X_, y_ = create_sequences(scaled_datasets[i], seq_length, pred_length)\n",
    "    X.append(X_)\n",
    "    y.append(y_)\n",
    "    np.save(f'datas/sequences/X_{tickers[i]}.npy', X_)\n",
    "    np.save(f'datas/sequences/y_{tickers[i]}.npy', y_)\n",
    "    print(f'{tickers[i]} verisi dizilere bölünerek kaydedildi')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için  seq2seq attention modeli oluştur\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(seq_length, len(features)))\n",
    "encoder_lstm = LSTM(100, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = RepeatVector(pred_length)(encoder_outputs)\n",
    "decoder_lstm = LSTM(100, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "# Dikkat Mekanizması\n",
    "attention_layer = Attention(use_scale=True)\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
    "\n",
    "\n",
    "# TimeDistributed Dense\n",
    "decoder_dense = TimeDistributed(Dense(len(features)))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "def create_model(encoder_inputs, decoder_outputs):\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    return model\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli oluştur ve eğit\n",
    "\n",
    "if not os.path.exists('datas/models'):\n",
    "    os.makedirs('datas/models')\n",
    "\n",
    "models = []\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    # modeli oluştur\n",
    "    model = create_model(encoder_inputs, decoder_outputs)\n",
    "    \n",
    "    # modeli derle\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # modeli listeye ekle\n",
    "    models.append(model)\n",
    "    \n",
    "    # modeli eğit\n",
    "    models[i].fit(X[i], y[i],\n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # modeli kaydet\n",
    "    models[i].save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# modelerden bir tanesini png kaydedelim\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(models[0], to_file='datas/model.png', show_shapes=True, show_layer_names=True , rankdir='TB', expand_nested=True, dpi=96, )"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için last_sequence = data_scaled[-seq_length:] mantığında prediction yap\n",
    "\n",
    "if not os.path.exists('datas/predictions'):\n",
    "    os.makedirs('datas/predictions')\n",
    "    \n",
    "predictions = []    \n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    last_sequence = scaled_datasets[i][-seq_length:]\n",
    "    last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    predicted = models[i].predict(last_sequence)\n",
    "    \n",
    "    predictions.append(predicted)\n",
    "    np.save(f'datas/predictions/{tickers[i]}_prediction.npy', predicted)\n",
    "    print(f'{tickers[i]} verisi için prediction yapıldı ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için predictionları geri dönüşüm yap ve yeni bi df e kaydet\n",
    "\n",
    "if not os.path.exists('datas/forecasts'):\n",
    "    os.makedirs('datas/forecasts')\n",
    "    \n",
    "forecasts = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    predicted = predictions[i]    \n",
    "    predicted = scaler.inverse_transform(predicted.reshape(-1, 1))\n",
    "    forecasts.append(predicted)\n",
    "    np.save(f'datas/forecasts/{tickers[i]}_forecast.npy', predicted)\n",
    "    print(f'{tickers[i]} verisi için forecast yapıldı ve kaydedildi')\n",
    "    \n",
    "# dataframe oluştur\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    forecast = forecasts[i]\n",
    "    forecast_dates = datasets[i].index[-pred_length:]\n",
    "    forecast = pd.DataFrame(forecast, index=forecast_dates, columns=[f'{tickers[i]} Forecast'])\n",
    "    forecast.to_csv(f'datas/forecasts/{tickers[i]}_forecast.csv')\n",
    "    print(f'{tickers[i]} verisi için forecast dataframe oluşturuldu ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#dataframeleri bi yazdır tarihleri ile\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    forecast = pd.read_csv(f'datas/forecasts/{tickers[i]}_forecast.csv', index_col=0)\n",
    "    print(f'{tickers[i]} verisi için forecast dataframe')\n",
    "    print(forecast)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Gerçek veriyi yazdır aynı tarihteki \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(f'{tickers[i]} verisi için gerçek veri')\n",
    "    print(datasets[i].tail(pred_length))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Kısım"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için eğitime uygun full data hazırla (tensor dataset)\n",
    "\n",
    "if not os.path.exists('datas/full_data'):\n",
    "    os.makedirs('datas/full_data')\n",
    "    \n",
    "full_data = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    \n",
    "    scaled_dataset_all = np.load(f'datas/scaled_datasets/{tickers[i]}.npy')\n",
    "    \n",
    "    scaled_dataset_all = tf.data.Dataset.from_tensor_slices(scaled_dataset_all)\n",
    "    \n",
    "    scaled_dataset_all = tf.data.Dataset.zip((scaled_dataset_all, scaled_dataset_all.skip(seq_length)))\n",
    "    \n",
    "    full_data.append(scaled_dataset_all)\n",
    "    \n",
    "    full_data[i] = full_data[i].batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    \n",
    "    full_data[i] = full_data[i].cache(f'datas/full_data/{tickers[i]}_full_data')\n",
    "\n",
    "    print(f'{tickers[i]} verisi için full data hazırlandı')\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli import et ve eğit\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    model = tf.keras.models.load_model(f'datas/models/{tickers[i]}_model.h5', custom_objects={'Attention': Attention})\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    model.fit(full_data[i], epochs=epochs, verbose=1)\n",
    "    \n",
    "    model.save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "into_future = 1\n",
    "\n",
    "def future_forecast(model, data, into_future):\n",
    "    future_forecast = []\n",
    "    last_sequence = data[-seq_length:]\n",
    "    last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    for i in range(into_future):\n",
    "        predicted = model.predict(last_sequence)\n",
    "        future_forecast.append(predicted)\n",
    "        last_sequence = np.append(last_sequence, predicted)\n",
    "        last_sequence = last_sequence[-seq_length:]\n",
    "        last_sequence = last_sequence.reshape((1, seq_length, len(features)))\n",
    "    return future_forecast\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print( into_future , seq_length,)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "future_values = []\n",
    "\n",
    "if not os.path.exists('datas/future_values'):\n",
    "    os.makedirs('datas/future_values')\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    future_values.append(future_forecast(models[i], scaled_datasets[i], into_future))\n",
    "    np.save(f'datas/future_values/{tickers[i]}_future_values.npy', future_values[i])\n",
    "    print(f'{tickers[i]} verisi için future values yapıldı ve kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için future valuesları geri dönüşüm yap ve yeni bi df e kaydet\n",
    "\n",
    "if not os.path.exists('datas/future_forecasts'):\n",
    "    os.makedirs('datas/future_forecasts')\n",
    "    \n",
    "future_forecasts = []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    \n",
    "    #np array olarak future values al\n",
    "    future_value = np.load(f'datas/future_values/{tickers[i]}_future_values.npy')\n",
    "    future_value = scaler.inverse_transform(future_value.reshape(-1, 1))\n",
    "    future_forecasts.append(future_value)\n",
    "    np.save(f'datas/future_forecasts/{tickers[i]}_future_forecast.npy', future_value)\n",
    "    print(f'{tickers[i]} verisi için future forecast yapıldı ve kaydedildi')\n",
    "    print(future_forecasts[i].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(future_forecasts[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(into_future) "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# dataframe oluştur\n",
    "\n",
    "if not os.path.exists('datas/future_forecasts_df'):\n",
    "    os.makedirs('datas/future_forecasts_df')\n",
    "\n",
    "hafta = (into_future*7) + 1\n",
    "\n",
    "for i in range(len(future_forecasts)):\n",
    "    future_forecast = future_forecasts[i]\n",
    "    future_forecast_dates = pd.date_range(start=datasets[i].index[-1], periods=hafta)[1:]\n",
    "    future_forecast = pd.DataFrame(future_forecast, index=future_forecast_dates, columns=features)\n",
    "    future_forecast.to_csv(f'datas/future_forecasts_df/{tickers[i]}_future_forecast.csv')\n",
    "    print(f'{tickers[i]} verisi için future forecast dataframe oluşturuldu ve kaydedildi')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
