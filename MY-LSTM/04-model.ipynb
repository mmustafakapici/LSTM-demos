{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yeni hali \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  LSTM , BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('datas'):\n",
    "    #sil\n",
    "    os.system('rm -rf datas')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = '3000d'\n",
    "\n",
    "#tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'GOOGL', 'TSLA', 'NVDA', 'PYPL', 'ADBE','BTC-USD', 'ETH-USD', 'XRP-USD', 'LTC-USD','BCH-USD', 'BNB-USD', 'LINK-USD', 'ADA-USD', 'XLM-USD', 'SOL-USD', 'TRX-USD']\n",
    "\n",
    "tickers = ['SOL-USD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indir_ve_df_olustur(tickers, period):\n",
    "    # Her bir hisse senedi için boş bir sözlük oluşturun\n",
    "    pariteler = {}\n",
    "\n",
    "    # Her bir hisse senedi için döngü oluşturun ve verileri indirin\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Hisse senedi verilerini indirin\n",
    "            veri = yf.download(ticker, period=period)\n",
    "            \n",
    "            # Veriyi sözlüğe ekleyin\n",
    "            pariteler[ticker] = veri\n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} için veri indirilirken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "    # Her bir hisse senedi için ayrı bir veri çerçevesi oluşturun\n",
    "    df_listesi = [veri for veri in pariteler.values()]\n",
    "\n",
    "    return df_listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pariteler adında klasör oluştur :\n",
    "\n",
    "import os \n",
    "\n",
    "if not os.path.exists('datas/pariteler'):\n",
    "    os.makedirs('datas/pariteler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = indir_ve_df_olustur(tickers, period)\n",
    "\n",
    "#pariteler içine kaydet \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].to_csv(f'datas/pariteler/{tickers[i]}.csv')\n",
    "    #print(f'{tickers[i]} verisi csv olarak kaydedildi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 30\n",
    "\n",
    "def veri_hazirla(df, window):\n",
    "    # Veri çerçevesinin kopyasını oluşturun\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Veri çerçevesine yeni sütunlar ekleyin\n",
    "    for i in range(1, window + 1):\n",
    "        df[f'Önceki_{i}_Açılış'] = df['Open'].shift(i)\n",
    "        df[f'Önceki_{i}_Yüksek'] = df['High'].shift(i)\n",
    "        df[f'Önceki_{i}_Düşük'] = df['Low'].shift(i)\n",
    "        df[f'Önceki_{i}_Kapanış'] = df['Close'].shift(i)\n",
    "        df[f'Önceki_{i}_Adj'] = df['Adj Close'].shift(i)\n",
    "        df[f'Önceki_{i}_Hacim'] = df['Volume'].shift(i)\n",
    "        \n",
    "    # NaN değerleri bırakın\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve verileri hazırlayın\n",
    "data_windowed = [veri_hazirla(df, window) for df in datasets]\n",
    "\n",
    "\n",
    "if not os.path.exists('datas/windowed'):\n",
    "    os.makedirs('datas/windowed')\n",
    "\n",
    "# Her bir veri çerçevesi için parite ismiyle birlikte windowed klasörüne kaydedin \n",
    "for i in range(len(data_windowed)):\n",
    "    data_windowed[i].to_csv(f'datas/windowed/{tickers[i]}_windowed.csv')\n",
    "    print(f'{tickers[i]} verisi windowed olarak kaydedildi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#her bir veri çerçevesi için X  = features(öncekiler) y = labels olan veriyapısını kuruyoruz (labels  = ['Open','High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "def X_y_olustur(df, window, labels):\n",
    "    # Veri çerçevesinin kopyasını oluşturun\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Özellikler ve etiketler için boş listeler oluşturun\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Her bir satır için döngü oluşturun\n",
    "    for i in range(len(df) - window):\n",
    "        # Özellikler için satırı alın\n",
    "        X_row = df.iloc[i:i + window].values\n",
    "        \n",
    "        # Etiketler için satırı alın\n",
    "        y_row = df[labels].iloc[i + window].values\n",
    "        \n",
    "        # Özellikleri ve etiketleri listelere ekleyin\n",
    "        X.append(X_row)\n",
    "        y.append(y_row)\n",
    "        \n",
    "    # Listeleri numpy dizilerine dönüştürün\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve X ve y oluşturun\n",
    "X_y = [X_y_olustur(df, window, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']) for df in data_windowed]\n",
    "#X_y = [X_y_olustur(df, window, ['Close' ,'Volume']) for df in data_windowed]\n",
    "\n",
    "# Her bir veri çerçevesi için parite ismiyle birlikte numpy klasörüne kaydedin\n",
    "\n",
    "if not os.path.exists('datas/numpy'):\n",
    "    os.makedirs('datas/numpy')\n",
    "for i in range(len(X_y)):\n",
    "    np.save(f'datas/numpy/{tickers[i]}_X.npy', X_y[i][0])\n",
    "    np.save(f'datas/numpy/{tickers[i]}_y.npy', X_y[i][1])\n",
    "    print(f'{tickers[i]} verisi X ve y olarak kaydedildi')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için train test split yapalım time series için doğru yolla yapmak gerekiyor\n",
    "\n",
    "def train_test_split(X, y, test_size):\n",
    "        # Test boyutunu hesaplayın\n",
    "\n",
    "        \n",
    "        #test_size = int(len(X) * test_size) #yüzdelikli tercih değil\n",
    "\n",
    "        \n",
    "        # Eğitim ve test veri kümelerini ayırın\n",
    "        X_train = X[:-test_size]\n",
    "        X_test = X[-test_size:]\n",
    "        y_train = y[:-test_size]\n",
    "        y_test = y[-test_size:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve train ve test veri kümelerini ayırın\n",
    "test_size = 60\n",
    "X_train_test = [train_test_split(X_y[i][0], X_y[i][1], 60) for i in range(len(X_y))]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için zerobase normalizasyon ve minmax scaling yapalım\n",
    "\n",
    "def normalize(X_train, X_test, y_train, y_test):\n",
    "    # Zerobase normalizasyonu uygulayın\n",
    "    X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "    y_train = (y_train - y_train.mean()) / y_train.std()\n",
    "    y_test = (y_test - y_test.mean()) / y_test.std()\n",
    "    \n",
    "    # Minmax scaling uygulayın\n",
    "    X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "    X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())\n",
    "    y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "    y_test = (y_test - y_test.min()) / (y_test.max() - y_test.min())\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve normalizasyon ve ölçeklendirme uygulayın\n",
    "\n",
    "X_train_test_normalized = [normalize(X_train_test[i][0], X_train_test[i][1], X_train_test[i][2], X_train_test[i][3]) for i in range(len(X_train_test))]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için LSTM modeli oluşturalım\n",
    "\n",
    "def build_model(input_shape ,  output_shape):\n",
    "    # Modeli oluşturun\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(128 , return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(output_shape))\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli eğitelim\n",
    "\n",
    "if not os.path.exists('datas/models'):\n",
    "    os.makedirs('datas/models')\n",
    "\n",
    "models=[]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(len(X_train_test_normalized)):\n",
    "    # Modeli oluşturun\n",
    "    model = build_model(X_train_test_normalized[i][0].shape[1:], X_train_test_normalized[i][2].shape[1])\n",
    "    \n",
    "    # Modeli derleyin\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Modeli eğitin\n",
    "    model.fit(X_train_test_normalized[i][0], X_train_test_normalized[i][2], batch_size=batch_size, epochs=epochs, validation_data=(X_train_test_normalized[i][1], X_train_test_normalized[i][3]))\n",
    "    \n",
    "    # Modeli kaydedin\n",
    "    model.save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    # Modeli listeye ekleyin\n",
    "    models.append(model)\n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli yükleyelim ve tahmin yapalım\n",
    "\n",
    "# Her bir model için döngü oluşturun ve tahmin yapın\n",
    "\n",
    "if not os.path.exists('datas/predictions'):\n",
    "    os.makedirs('datas/predictions')\n",
    "    \n",
    "for i in range(len(models)):\n",
    "    # Modeli yükleyin\n",
    "    model = tf.keras.models.load_model(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    # Tahmin yapın\n",
    "    predictions = model.predict(X_train_test_normalized[i][1])\n",
    "    \n",
    "    # Tahminleri kaydedin\n",
    "    np.save(f'datas/predictions/{tickers[i]}_predictions.npy', predictions)\n",
    "    print(f'{tickers[i]} için tahminler kaydedildi')\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için tahminleri gerçek verilerle karşılaştıralım\n",
    "\n",
    "# Her bir model için döngü oluşturun ve tahminleri gerçek verilerle karşılaştırın\n",
    "\n",
    "for i in range(len(X_train_test_normalized)):\n",
    "    # Gerçek verileri yükleyin\n",
    "    y_test = X_train_test_normalized[i][3]\n",
    "    \n",
    "    # Tahminleri yükleyin\n",
    "    predictions = np.load(f'datas/predictions/{tickers[i]}_predictions.npy')\n",
    "    \n",
    "    # Gerçek verileri ters ölçeklendirin\n",
    "    y_test = (y_test * (y_test.max() - y_test.min())) + y_test.min()\n",
    "    \n",
    "    # Tahminleri ters ölçeklendirin\n",
    "    predictions = (predictions * (y_test.max() - y_test.min())) + y_test.min()\n",
    "    \n",
    "    # Gerçek verileri ve tahminleri çizdirin\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test, label='Gerçek Veriler')\n",
    "    plt.plot(predictions, label='Tahminler')\n",
    "    plt.title(f'{tickers[i]} Gerçek Veriler ve Tahminler')\n",
    "    plt.xlabel('Zaman')\n",
    "    plt.ylabel('Fiyat')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#TODO :  normalizasyon yapılacak : zerobase - minmax scaler  +\n",
    "\n",
    "#TODO : seq2seq LSTM Yapılcak denenecek\n",
    "\n",
    "#TODO : Dikkat Mekanizması ile LSTM yapılacak"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
