{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yeni hali \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  LSTM , BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = '3000d'\n",
    "\n",
    "#tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'GOOGL', 'TSLA', 'NVDA', 'PYPL', 'ADBE','BTC-USD', 'ETH-USD', 'XRP-USD', 'LTC-USD','BCH-USD', 'BNB-USD', 'LINK-USD', 'ADA-USD', 'XLM-USD', 'SOL-USD', 'TRX-USD']\n",
    "\n",
    "tickers = ['SOL-USD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indir_ve_df_olustur(tickers, period):\n",
    "    # Her bir hisse senedi için boş bir sözlük oluşturun\n",
    "    pariteler = {}\n",
    "\n",
    "    # Her bir hisse senedi için döngü oluşturun ve verileri indirin\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Hisse senedi verilerini indirin\n",
    "            veri = yf.download(ticker, period=period)\n",
    "            \n",
    "            # Veriyi sözlüğe ekleyin\n",
    "            pariteler[ticker] = veri\n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} için veri indirilirken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "    # Her bir hisse senedi için ayrı bir veri çerçevesi oluşturun\n",
    "    df_listesi = [veri for veri in pariteler.values()]\n",
    "\n",
    "    return df_listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pariteler adında klasör oluştur :\n",
    "\n",
    "import os \n",
    "\n",
    "if not os.path.exists('datas/pariteler'):\n",
    "    os.makedirs('datas/pariteler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = indir_ve_df_olustur(tickers, period)\n",
    "\n",
    "#pariteler içine kaydet \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].to_csv(f'datas/pariteler/{tickers[i]}.csv')\n",
    "    #print(f'{tickers[i]} verisi csv olarak kaydedildi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 30\n",
    "\n",
    "def veri_hazirla(df, window):\n",
    "    # Veri çerçevesinin kopyasını oluşturun\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Veri çerçevesine yeni sütunlar ekleyin\n",
    "    for i in range(1, window + 1):\n",
    "        df[f'Önceki_{i}_Açılış'] = df['Open'].shift(i)\n",
    "        df[f'Önceki_{i}_Yüksek'] = df['High'].shift(i)\n",
    "        df[f'Önceki_{i}_Düşük'] = df['Low'].shift(i)\n",
    "        df[f'Önceki_{i}_Kapanış'] = df['Close'].shift(i)\n",
    "        df[f'Önceki_{i}_Adj'] = df['Adj Close'].shift(i)\n",
    "        df[f'Önceki_{i}_Hacim'] = df['Volume'].shift(i)\n",
    "        \n",
    "    # NaN değerleri bırakın\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve verileri hazırlayın\n",
    "data_windowed = [veri_hazirla(df, window) for df in datasets]\n",
    "\n",
    "\n",
    "if not os.path.exists('datas/windowed'):\n",
    "    os.makedirs('datas/windowed')\n",
    "\n",
    "# Her bir veri çerçevesi için parite ismiyle birlikte windowed klasörüne kaydedin \n",
    "for i in range(len(data_windowed)):\n",
    "    data_windowed[i].to_csv(f'datas/windowed/{tickers[i]}_windowed.csv')\n",
    "    print(f'{tickers[i]} verisi windowed olarak kaydedildi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#her bir veri çerçevesi için X  = features(öncekiler) y = labels olan veriyapısını kuruyoruz (labels  = ['Open','High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "\n",
    "def X_y_olustur(df, window, labels):\n",
    "    # Veri çerçevesinin kopyasını oluşturun\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Özellikler ve etiketler için boş listeler oluşturun\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Her bir satır için döngü oluşturun\n",
    "    for i in range(len(df) - window):\n",
    "        # Özellikler için satırı alın\n",
    "        X_row = df.iloc[i:i + window].values\n",
    "        \n",
    "        # Etiketler için satırı alın\n",
    "        y_row = df[labels].iloc[i + window].values\n",
    "        \n",
    "        # Özellikleri ve etiketleri listelere ekleyin\n",
    "        X.append(X_row)\n",
    "        y.append(y_row)\n",
    "        \n",
    "    # Listeleri numpy dizilerine dönüştürün\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve X ve y oluşturun\n",
    "X_y = [X_y_olustur(df, window, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']) for df in data_windowed]\n",
    "#X_y = [X_y_olustur(df, window, ['Close' ,'Volume']) for df in data_windowed]\n",
    "\n",
    "# Her bir veri çerçevesi için parite ismiyle birlikte numpy klasörüne kaydedin\n",
    "\n",
    "if not os.path.exists('datas/numpy'):\n",
    "    os.makedirs('datas/numpy')\n",
    "for i in range(len(X_y)):\n",
    "    np.save(f'datas/numpy/{tickers[i]}_X.npy', X_y[i][0])\n",
    "    np.save(f'datas/numpy/{tickers[i]}_y.npy', X_y[i][1])\n",
    "    print(f'{tickers[i]} verisi X ve y olarak kaydedildi')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için train test split yapalım time series için doğru yolla yapmak gerekiyor\n",
    "\n",
    "def train_test_split(X, y, test_size):\n",
    "        # Test boyutunu hesaplayın\n",
    "\n",
    "        \n",
    "        #test_size = int(len(X) * test_size) #yüzdelikli tercih değil\n",
    "\n",
    "        \n",
    "        # Eğitim ve test veri kümelerini ayırın\n",
    "        X_train = X[:-test_size]\n",
    "        X_test = X[-test_size:]\n",
    "        y_train = y[:-test_size]\n",
    "        y_test = y[-test_size:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve train ve test veri kümelerini ayırın\n",
    "test_size = 60\n",
    "X_train_test = [train_test_split(X_y[i][0], X_y[i][1], 60) for i in range(len(X_y))]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için normalizasyon ve Scaling yapalım\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalizasyon_yap(X_train, X_test, y_train, y_test):\n",
    "    # Normalizasyon için ölçekleyici oluşturun\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    # Eğitim veri kümelerini ölçekleyin\n",
    "    X_train = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    y_train = scaler_y.fit_transform(y_train.reshape(-1, y_train.shape[-1])).reshape(y_train.shape)\n",
    "    \n",
    "    # Test veri kümelerini ölçekleyin\n",
    "    X_test = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    y_test = scaler_y.transform(y_test.reshape(-1, y_test.shape[-1])).reshape(y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve normalizasyon yapın\n",
    "\n",
    "X_train_test_normalized = [normalizasyon_yap(X_train_test[i][0], X_train_test[i][1], X_train_test[i][2], X_train_test[i][3]) for i in range(len(X_train_test))]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için model oluşturup eğitelim\n",
    "\n",
    "def model_olustur(input_shape, output_shape):\n",
    "    # Modeli oluşturun\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(output_shape))\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Her bir veri çerçevesi için döngü oluşturun ve modeli oluşturun ve eğitin\n",
    "\n",
    "if not os.path.exists('datas/models'):\n",
    "    os.makedirs('datas/models')\n",
    "\n",
    "models= []\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(len(X_train_test_normalized)):\n",
    "    # Giriş ve çıkış şeklini alın\n",
    "    input_shape = X_train_test_normalized[i][0].shape[1:]\n",
    "    output_shape = X_train_test_normalized[i][2].shape[1]\n",
    "    \n",
    "    # Modeli oluşturun\n",
    "    model = model_olustur(input_shape, output_shape)\n",
    "    \n",
    "    # Modeli derleyin\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Modeli eğitin\n",
    "    model.fit(X_train_test_normalized[i][0], X_train_test_normalized[i][2], batch_size=batch_size, epochs=epochs, validation_data=(X_train_test_normalized[i][1], X_train_test_normalized[i][3]))\n",
    "    \n",
    "    # Modeli kaydedin\n",
    "    model.save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    # Modeli listeye ekleyin\n",
    "    models.append(model)\n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için tahmin yapalım ve gerçekle karşılaştıralım\n",
    "\n",
    "# Her bir veri çerçevesi için döngü oluşturun ve tahmin yapın\n",
    "\n",
    "for i in range(len(X_train_test_normalized)):\n",
    "    # Modeli ve ölçekleyiciyi alın\n",
    "    model = models[i]\n",
    "    scaler_X = X_train_test_normalized[i][4]\n",
    "    scaler_y = X_train_test_normalized[i][5]\n",
    "    \n",
    "    # Tahmin yapın\n",
    "    tahmin = model.predict(X_train_test_normalized[i][1])\n",
    "    \n",
    "    # Tahminleri ölçekten çıkarın\n",
    "    tahmin = scaler_y.inverse_transform(tahmin)\n",
    "    \n",
    "    # Gerçek etiketleri ölçekten çıkarın\n",
    "    gercek = scaler_y.inverse_transform(X_train_test_normalized[i][3])\n",
    "    \n",
    "    # Tahminleri ve gerçek etiketleri çizdirin\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(gercek, color='red', label='Gerçek')\n",
    "    plt.plot(tahmin, color='blue', label='Tahmin')\n",
    "    plt.title(f'{tickers[i]} Tahmin vs Gerçek')\n",
    "    plt.xlabel('Zaman')\n",
    "    plt.ylabel('Fiyat')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f'{tickers[i]} için tahminler ve gerçekler çizdirildi')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO :  normalizasyon yapılacak : zerobase - minmax scaler  +\n",
    "\n",
    "#TODO : seq2seq LSTM Yapılcak denenecek\n",
    "\n",
    "#TODO : Dikkat Mekanizması ile LSTM yapılacak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
