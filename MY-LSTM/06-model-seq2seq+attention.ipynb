{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yeni hali \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  LSTM , BatchNormalization\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector, Concatenate, Attention\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('datas'):\n",
    "    #sil\n",
    "    os.system('rm -rf datas')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = '3000d'\n",
    "\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Close','Adj Close', 'Volume']\n",
    "#features = [ 'Close',]  # Sadece kapanış fiyatını kullanın\n",
    "\n",
    "#tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'GOOGL', 'TSLA', 'NVDA', 'PYPL', 'ADBE','BTC-USD', 'ETH-USD', 'XRP-USD', 'LTC-USD','BCH-USD', 'BNB-USD', 'LINK-USD', 'ADA-USD', 'XLM-USD', 'SOL-USD', 'TRX-USD']\n",
    "\n",
    "tickers = ['SOL-USD']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indir_ve_df_olustur(tickers, period):\n",
    "    # Her bir hisse senedi için boş bir sözlük oluşturun\n",
    "    pariteler = {}\n",
    "\n",
    "    # Her bir hisse senedi için döngü oluşturun ve verileri indirin\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Hisse senedi verilerini indirin\n",
    "            veri = yf.download(ticker, period=period)\n",
    "\n",
    "            veri = veri[features]   \n",
    "            # Veriyi sözlüğe ekleyin\n",
    "            pariteler[ticker] = veri\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ticker} için veri indirilirken bir hata oluştu: {str(e)}\")\n",
    "\n",
    "    # Her bir hisse senedi için ayrı bir veri çerçevesi oluşturun\n",
    "    df_listesi = [veri for veri in pariteler.values()]\n",
    "\n",
    "    return df_listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pariteler adında klasör oluştur :\n",
    "\n",
    "import os \n",
    "\n",
    "if not os.path.exists('datas/pariteler'):\n",
    "    os.makedirs('datas/pariteler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = indir_ve_df_olustur(tickers, period)\n",
    "\n",
    "#pariteler içine kaydet \n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i].to_csv(f'datas/pariteler/{tickers[i]}.csv')\n",
    "    print(f'{tickers[i]} verisi csv olarak kaydedildi')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler  = MinMaxScaler()\n",
    "\n",
    "#her bir veri çerçevesi için scaled dataset oluştur\n",
    "\n",
    "if not os.path.exists('datas/scaled_datasets'):\n",
    "    os.makedirs('datas/scaled_datasets')\n",
    "\n",
    "scaled_datasets = []\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    scaled_datasets.append(scaler.fit_transform(datasets[i]))\n",
    "    np.save(f'datas/scaled_datasets/{tickers[i]}.npy', scaled_datasets[i])\n",
    "    print(f'{tickers[i]} verisi scaled edilerek kaydedildi')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "seq_length = 60  # Son 60 günü kullanarak\n",
    "pred_length = 15  # Önümüzdeki 15 günü tahmin etme\n",
    "\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length-pred_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# her bir veri çerçevesi için veriyi dizilere bölme\n",
    "\n",
    "if not os.path.exists('datas/sequences'):\n",
    "    os.makedirs('datas/sequences')\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    X_, y_ = create_sequences(scaled_datasets[i], seq_length, pred_length)\n",
    "    X.append(X_)\n",
    "    y.append(y_)\n",
    "    np.save(f'datas/sequences/X_{tickers[i]}.npy', X_)\n",
    "    np.save(f'datas/sequences/y_{tickers[i]}.npy', y_)\n",
    "    print(f'{tickers[i]} verisi dizilere bölünerek kaydedildi')\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için veriyi train ve test olarak bölme\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    X_train_, X_test_, y_train_, y_test_ = X[i][:-pred_length], X[i][-pred_length:], y[i][:-pred_length], y[i][-pred_length:]\n",
    "    X_train.append(X_train_)\n",
    "    X_test.append(X_test_)\n",
    "    y_train.append(y_train_)\n",
    "    y_test.append(y_test_)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için  seq2seq attention modeli oluştur\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(seq_length, len(features)))\n",
    "encoder_lstm = LSTM(100, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = RepeatVector(pred_length)(encoder_outputs)\n",
    "decoder_lstm = LSTM(100, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "# Dikkat Mekanizması\n",
    "attention_layer = Attention(use_scale=True)\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
    "\n",
    "\n",
    "# TimeDistributed Dense\n",
    "decoder_dense = TimeDistributed(Dense(len(features)))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "def create_model(encoder_inputs, decoder_outputs):\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    return model\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# her bir veri çerçevesi için modeli oluştur ve eğit\n",
    "\n",
    "if not os.path.exists('datas/models'):\n",
    "    os.makedirs('datas/models')\n",
    "\n",
    "models = []\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    # modeli oluştur\n",
    "    model = create_model(encoder_inputs, decoder_outputs)\n",
    "    \n",
    "    # modeli derle\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # modeli listeye ekle\n",
    "    models.append(model)\n",
    "    \n",
    "    # modeli eğit\n",
    "    models[i].fit(X_train[i], y_train[i], batch_size=batch_size, epochs=epochs, validation_data=(X_test[i], y_test[i]))\n",
    "    \n",
    "    # modeli kaydet\n",
    "    models[i].save(f'datas/models/{tickers[i]}_model.h5')\n",
    "    print(f'{tickers[i]} modeli eğitildi ve kaydedildi')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# modelerden bir tanesini png kaydedelim\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(models[0], to_file='datas/model.png', show_shapes=True, show_layer_names=True , rankdir='TB', expand_nested=True, dpi=96, )"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Son veri noktasını kullanarak geleceği tahmin etme\n",
    "\n",
    "if not os.path.exists('datas/predictions'):\n",
    "    os.makedirs('datas/predictions')\n",
    "    \n",
    "    \n",
    "for i in range(len(scaled_datasets)):\n",
    "    # modeli yükle\n",
    "    model = tf.keras.models.load_model(f'datas/models/{tickers[i]}_model.h5')\n",
    "    \n",
    "    # geleceği tahmin et\n",
    "    predicted = model.predict(X_test[i])\n",
    "    \n",
    "    # tahminleri kaydet\n",
    "    np.save(f'datas/predictions/{tickers[i]}_predictions.npy', predicted)\n",
    "    print(f'{tickers[i]} için tahminler kaydedildi')\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tahminleri gerçek verilerle karşılaştır\n",
    "\n",
    "for i in range(len(scaled_datasets)):\n",
    "    # gerçek verileri yükle\n",
    "    real = y_test[i]\n",
    "    \n",
    "    # tahminleri yükle\n",
    "    predicted = np.load(f'datas/predictions/{tickers[i]}_predictions.npy')\n",
    "    \n",
    "    # gerçek ve tahmin edilen verileri ters ölçeklendir\n",
    "    real = scaler.inverse_transform(real.reshape(-1, len(features)))\n",
    "    predicted = scaler.inverse_transform(predicted.reshape(-1, len(features)))\n",
    "    \n",
    "    # gerçek ve tahmin edilen verileri veri çerçevesine dönüştür\n",
    "    real_df = pd.DataFrame(real, columns=features)\n",
    "    predicted_df = pd.DataFrame(predicted, columns=features)\n",
    "    \n",
    "    # gerçek ve tahmin edilen tüm verileri çizdir\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for feature in features:\n",
    "        plt.plot(real_df[feature], label=f'Real {feature}')\n",
    "        plt.plot(predicted_df[feature], label=f'Predicted {feature}')\n",
    "    plt.title(f'{tickers[i]} Real vs Predicted')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(f'datas/charts/{tickers[i]}_real_vs_predicted.png')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
